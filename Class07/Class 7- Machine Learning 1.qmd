---
title: "Class 7: Machine Learning 1"
author: "Lilith Sadil, A16470107"
format: pdf
---

Today, we'll start out multi-part exploration of some key machine learning methods. We'll begin with clustering (a way of bunching/grouping data based off of similarity/patterns and then using dimensional reduction).

## Clustering

Let's start with "k-means" clustering. In this approach, you define how many groups (# = k) you want and then the computer bunches the data you provide into k groups

The main function in base R for this is `kmeans()`.

```{r}
#Make up some data; here, 1000 points will be randomly generated along a normal distribution and assigned to the variable "numbers"
numbers1 = rnorm(1000)
# we can turn this data into a histogram:
hist(numbers1)
```

```{r}
#we can also make a histogram with a shifted mean - at 3 instead of 0 
numbers2 = rnorm(1000, mean=3)
hist(numbers2)
```

Next, we'll make two data sets of 30 points - one is centered around -3 and the other around +3

```{r}
rnorm(30,-3)
rnorm(30,3)
```

Next, we'll concatenate these two data sets/vectors into one data set

```{r}
combined_data = c(rnorm(30,-3), rnorm(30,3))
```

If we want to print the values centered around +3 first, we can reverse the order of the vector

```{r}
combined_data = c(rnorm(30,-3), rnorm(30,3))
xy_data = cbind(x=combined_data, y=rev(combined_data))
plot(xy_data)
```

Now, we'll use kmeans to analyse these groups

```{r}
km = kmeans(xy_data, centers=2)
#in the parentheses of kmeans, we input x (numeric data matrix we're analysing) and centers (the number of clusters we want,k)
km
#the center of each cluster group is located at (3.11,-2.97) and (-2.97, 3.11)
```

When we use the k-means operation, we are able to access 9 different pieces of information/components regarding the data set - one is "size" which tells us the size of each cluster

> Q. How many points in each cluster?

```{r}
km$size
```

> Q. What component of your result object details cluster allignment/membership?

```{r}
km$cluster
```

> Q. What are centers/mean values of each cluster?

```{r}
km$centers
```

> Q. Make a plot of the data showing clustering results.

Here, we want to separate out our data clusters based on color. How do we do this? By default, if only 2 color values are assigned, then those colors will be "recycled" to alternate the color of every point.

```{r}
plot(xy_data, col=c("#AF929D", "#615055"))
```

To assign color by clusters, we have to split the data clusters into their respective vectors and color them (items=1 are black, items=2 are red). Next, we'll mark the cluster centers with a pink dot

```{r}
plot(xy_data, col=km$cluster)
points(km$centers, col="#AF929D", pch=20, cex=2)
```

> Q. Run `kmeans()` again and cluster into four groups then plot.

```{r}
kmeans(xy_data, centers=4)
```

```{r}
km4 = kmeans(xy_data, centers=4)
plot(xy_data, col=km4$cluster)
points(km4$centers, col="#AF929D", pch=20, cex=2)
```

Here, the clusters aren't as distinct as before - however, 4 clusters were formed since we told the computer it had to.

## Hierarchical Clustering

This form of clustering aims to reveal the structure in your data by progressively grouping points into an ever smaller number of clusters.

The main function in base R for this is called `hclust()`. This function doesn't take our input data directly; it requires a "distance matrix" that details how dis/similar all out input points are to each other.

We can use the `dist()` function on our xy_data dataset in order to get the distance between every pair of points in the dataset

```{r}
dist(xy_data)
```
Now, we'll assign the resulting distance matrix to a variable, hc, and apply heirarchical clustering 
```{r}
hc = hclust(dist(xy_data))
hc
```

The resulting information isn't very useful on its own; let's plot it:
```{r}
plot(hc)
```
The vertical height of the dendrogram shows the similarity of two clusters through the vertical height that separates the crossbar between them. The two major clusters on the left and right sides are split by the first and last 30 data points; the 1st 30 make up the 1st cluster & points 30-60 create the second cluster.

The biggest "goalpost" indicates the optimal separation of clusters (ex. the top-most crossbar separates two clusters with a very larger vertical distance; the left and right groups are good clusterings).

We can create an upper limit or cutoff line, we can insert an `abline()` at our desired y-value/height.

```{r}
plot(hc)
abline(h=10,col="red")
```

Now, we can cut the tree at a y-value of 10. Everything clustered under that line gets assigned to a cluster (cluster1 assigned a value of 1, cluster2 assigned a value of 2).
```{r}
grps = cutree(hc, h=10)
grps
```

Now, we can plot the original data (xy_data) with the colors split based off of the clustering we did above (grps).
```{r}
plot(xy_data, col=grps)
# alternatively, we can use: plot(xy_data, col=cutree(hc, h=10))
```

## Principal Component Analysis

The goal of PCA is to reduce the dimensionality of a dataset down to some smaller subset of new variables (called PCs) which are useful bases for further analysis - like visualization, clustering, etc.

In this part of the module, we'll look at a way of reading the food consumption of individuals across 4 countries in the UK, across 17 different categories (potato, cheese, fruit...).

First, we'll import the dataset:
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1) # saying row.names=1 means that we start counting the columns at england not at the cheese/carcass_meat... column - we get 4 columns instead of 5 
x
```

To look at the first few lines of the datset:
```{r}
head(x)
```



>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x) # or separately,
ncol(x)
nrow(x)
```
We can make a barplot which compares the consumption of each of the 17 food categories across the 4 countries; not very readable without PCA since there are too many dimensions to work with.
```{r}
barplot(as.matrix(x), col=rainbow(nrow(x)))
#using rainbow(nrow(x)) gives us one color of the rainbow for each of the 17 rows in x
```
We can also plot each of these relationships pairwise (ex. england is the y-axis for the top, rightmost graph and n.ireland is the x-axis); below and above the diagnoal are the same. For each graph, we can draw a diagonal line across the plot; any departure from the straight line means that the two countries being compared differ for that data point (ex. the scotland v. north ireland plot shows that food consumption associated with the green dot is different). Pairs plots may be useful for small datasets but are less readable for large datasets.
```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```

PCA will help make this data comparison more readable:

The main function to do this in base R is `prcomp()`

```{r}
t(x) #this transposes the matrix so that the food act as columns and the countries are the rows (inverts the data)
pca = prcomp(t(x)) #apple PCA to the transposed food/country matrix
summary(pca) #gives a summary of the pca analysis 
```
Note how PC1 always has the most variable (largest standard deviation); proportion of variance tells us that PC1 (a 1 dimensional line across the data) captures the broadest/largest amount of the variance in the data (67.44%). PC2 adds a second dimension and captures an additional 29.05% of variance in the data. Adding a third dimension, PC3, captures an additional 3.5% of variance in the data. Together, all 3 dimensions cover ~100% of the data's variance. PCA1 is the most important since it captures the greatest amount of variance in the data/

```{r}
pca$x
```

A major PCA result vizualization is called a "PCA plot" (a.k.a. a score plot/biplot/PC1 v PC2 plot/ordination plot - depending on the field of data analysis you're working in).

Now, we'll plot PCA1 vs PCA2. By doing so, we see that there's a large outliar in the data set (n.ireland, dark green

```{r}
mycols = c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, cex=2)

```
We can also plot this using the names of each country
```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

Now, we see that ireland is very different from the other countries. But how? What categories of food consumption set it aside from the rest of the UK?

---
Another important output from PCA is called the "loadings" vector or the "rotation" component - it tells us how much the original variables (here, the food categories) contribute to the new PCs.

```{r}
pca$rotation
```
Some values are positive and negative; positive values for a certain category means that that food is consumed more than the other countries; negative values say the opposite (i.e that country eats less of a particular food). Ireland, for example, had a positive 0.40 value for fresh potatoes


Here, we've added in axes which shows PCA1 vs PCA2 
```{r}
mycols = c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, cex=2)
abline(h=0, col="grey")
abline(v=0, col="grey")
```

We'll leave it off here for this class, but the main takewaway for this section is that PCS is a useful method for gaining some insight into data with many dimensions which are difficult to examine in other ways.

# PCA Of RNASeq Data
 
## Data Input

```{r}
url2 = "https://tinyurl.com/expression-CSV"
rna.data = read.csv(url2, row.names=1)
head(rna.data)
```

wt stands for "wild type" and ko stands for "knockout" 

```{r}
pca = prcomp(t(rna.data), scale=TRUE) #scale=true makes sure that the data is scaled
summary(pca)
```
92.3% of variance in the original data set is captured by PC1 alone. 

>Q. How many genes are in the dataset?

```{r}
nrow(rna.data)
```

```{r}
attributes(pca)
```
pca$x tells us the coordinates of the calculated PC1,2,3... for each variable or column in the dataset; using this, we can plot the points to see how similar or different each gene is to one-another
```{r}
head(pca$x)
```

Now, we'll make a plot of the pca$x data in ggplot2

```{r}
library(ggplot2)
```

```{r}
res = as.data.frame(pca$x)
head(res)
```
Now, we'll plot PC1 vs PC2. Note that separation of points on the PC1 axis is far more significant than PC2 since PC1 accounts for 96% of the data's variance (PC2 is onlt ~2% of the variance).

```{r}
ggplot(res)+
  aes(x=PC1,y=PC2)+
  geom_point()
```
We can also use `kmeans` on the first column of pca$x (i.e. PCA1) to determine the two categories which we can sort the genes into (the dots on the left and right halves of the plot above).
```{r}
kmeans(pca$x[,1], centers=2)
```



